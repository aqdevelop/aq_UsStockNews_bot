#!/usr/bin/env python3
"""
ì£¼ê°„ í•« ë‰´ìŠ¤ ë¶„ì„ê¸°
Reddit WSB + Google Trends + 7ì¼ì¹˜ ë‰´ìŠ¤ ê¸°ë¡ â†’ GPT ë¶„ì„ â†’ TOP 10
"""

import json
import os
import re
from datetime import datetime, timedelta
from collections import Counter
from typing import List, Dict
import requests

# Reddit & Google Trends
try:
    import praw
    REDDIT_AVAILABLE = True
except ImportError:
    REDDIT_AVAILABLE = False
    print("âš ï¸ Reddit ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—†ìŒ - Reddit ë¶„ì„ ìŠ¤í‚µ")

try:
    from pytrends.request import TrendReq
    TRENDS_AVAILABLE = True
except ImportError:
    TRENDS_AVAILABLE = False
    print("âš ï¸ Google Trends ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—†ìŒ - Trends ë¶„ì„ ìŠ¤í‚µ")


class WeeklyHotNewsAnalyzer:
    def __init__(self, openai_api_key: str, sent_news_file: str):
        self.openai_api_key = openai_api_key
        self.sent_news_file = sent_news_file
        
        # Reddit ì„¤ì • (í™˜ê²½ ë³€ìˆ˜ì—ì„œ ê°€ì ¸ì˜¤ê¸°)
        self.reddit_client_id = os.getenv('REDDIT_CLIENT_ID')
        self.reddit_client_secret = os.getenv('REDDIT_CLIENT_SECRET')
        
    def _load_weekly_news_history(self) -> List[Dict]:
        """ì§€ë‚œ 7ì¼ê°„ ì „ì†¡ëœ ë‰´ìŠ¤ ê¸°ë¡ ë¡œë“œ"""
        try:
            if not os.path.exists(self.sent_news_file):
                print("âš ï¸ ë‰´ìŠ¤ ê¸°ë¡ íŒŒì¼ ì—†ìŒ")
                return []
            
            with open(self.sent_news_file, 'r', encoding='utf-8') as f:
                history = json.load(f)
            
            # 7ì¼ ì „ ë‚ ì§œ ê³„ì‚°
            seven_days_ago = datetime.now() - timedelta(days=7)
            
            # 7ì¼ ì´ë‚´ ë‰´ìŠ¤ë§Œ í•„í„°ë§
            weekly_news = [
                news for news in history.get('sent_news', [])
                if news.get('sent_at', '') > seven_days_ago.isoformat()
            ]
            
            print(f"ğŸ“Š ì§€ë‚œ 7ì¼ê°„ ì „ì†¡ëœ ë‰´ìŠ¤: {len(weekly_news)}ê°œ")
            return weekly_news
            
        except Exception as e:
            print(f"âš ï¸ ë‰´ìŠ¤ ê¸°ë¡ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return []
    
    def get_reddit_wsb_hot_tickers(self, limit: int = 100) -> Dict[str, int]:
        """Reddit r/wallstreetbetsì—ì„œ í•«í•œ í‹°ì»¤ ì¶”ì¶œ"""
        if not REDDIT_AVAILABLE:
            print("âš ï¸ Reddit ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—†ìŒ - ìŠ¤í‚µ")
            return {}
        
        if not self.reddit_client_id or not self.reddit_client_secret:
            print("âš ï¸ Reddit API í‚¤ ì—†ìŒ - ìŠ¤í‚µ")
            return {}
        
        try:
            print(f"\nğŸ” Reddit r/wallstreetbets ë¶„ì„ ì¤‘...")
            
            reddit = praw.Reddit(
                client_id=self.reddit_client_id,
                client_secret=self.reddit_client_secret,
                user_agent='US Stock Bot v1.0'
            )
            
            ticker_counts = Counter()
            ticker_contexts = {}  # í‹°ì»¤ë³„ ëŒ€í‘œ ì œëª© ì €ì¥
            
            # ê³µí†µ ë‹¨ì–´ í•„í„° (í‹°ì»¤ ì•„ë‹Œ ê²ƒë“¤)
            common_words = {
                'TO', 'FOR', 'THE', 'AND', 'OR', 'BUT', 'NOT', 'ARE', 'WAS',
                'HAS', 'HAD', 'CAN', 'ALL', 'NEW', 'NOW', 'OUT', 'ANY', 'WHO',
                'HOW', 'WHY', 'GET', 'GOT', 'SEE', 'SAW', 'WAY', 'OUR', 'YOU',
                'YOUR', 'WILL', 'WOULD', 'COULD', 'SHOULD', 'MAY', 'MIGHT',
                'BEEN', 'BEING', 'HAVE', 'HIS', 'HER', 'ITS', 'THEIR', 'THERE',
                'WHAT', 'WHEN', 'WHERE', 'WHICH', 'THIS', 'THAT', 'THESE', 'THOSE',
                'FROM', 'WITH', 'INTO', 'OVER', 'AFTER', 'BEFORE', 'ABOUT',
                'AGAINST', 'BETWEEN', 'DURING', 'WITHOUT', 'THROUGH', 'THAN',
                'USA', 'CEO', 'IPO', 'ETF', 'WSB', 'YOLO', 'DD', 'TA', 'IMO'
            }
            
            # Hot í¬ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
            subreddit = reddit.subreddit('wallstreetbets')
            hot_posts = subreddit.hot(limit=limit)
            
            for post in hot_posts:
                # í‹°ì»¤ íŒ¨í„´: $TSLA ë˜ëŠ” TSLA (ëŒ€ë¬¸ì 2-5ê¸€ì)
                text = post.title + " " + post.selftext
                tickers = re.findall(r'\$?([A-Z]{2,5})\b', text)
                
                # í•„í„°ë§
                valid_tickers = [
                    t for t in tickers 
                    if t not in common_words and t.isalpha()
                ]
                
                # ì¹´ìš´íŠ¸ ì¦ê°€
                for ticker in valid_tickers:
                    ticker_counts[ticker] += 1
                    
                    # ëŒ€í‘œ ì œëª© ì €ì¥ (upvote ë†’ì€ ê²ƒ)
                    if ticker not in ticker_contexts or post.score > ticker_contexts[ticker]['score']:
                        ticker_contexts[ticker] = {
                            'title': post.title,
                            'score': post.score,
                            'url': f"https://reddit.com{post.permalink}"
                        }
            
            # ìƒìœ„ í‹°ì»¤ë§Œ ë°˜í™˜
            top_tickers = dict(ticker_counts.most_common(20))
            
            print(f"âœ… Reddit ë¶„ì„ ì™„ë£Œ: {len(top_tickers)}ê°œ í‹°ì»¤ ë°œê²¬")
            for ticker, count in list(top_tickers.items())[:5]:
                print(f"   {ticker}: {count}íšŒ ì–¸ê¸‰")
            
            return {
                'tickers': top_tickers,
                'contexts': ticker_contexts
            }
            
        except Exception as e:
            print(f"âš ï¸ Reddit ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {}
    
    def get_google_trends_data(self, tickers: List[str]) -> Dict[str, int]:
        """Google Trendsì—ì„œ ì£¼ì‹ í‹°ì»¤ ê²€ìƒ‰ëŸ‰ í™•ì¸"""
        if not TRENDS_AVAILABLE:
            print("âš ï¸ Google Trends ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—†ìŒ - ìŠ¤í‚µ")
            return {}
        
        if not tickers:
            return {}
        
        try:
            print(f"\nğŸ“Š Google Trends ë¶„ì„ ì¤‘... ({len(tickers)}ê°œ í‹°ì»¤)")
            
            # í•œ ë²ˆì— ìµœëŒ€ 5ê°œì”©ë§Œ ì¡°íšŒ ê°€ëŠ¥
            pytrends = TrendReq(hl='en-US', tz=360)
            trends_data = {}
            
            # 5ê°œì”© ë¬¶ì–´ì„œ ì²˜ë¦¬
            for i in range(0, len(tickers), 5):
                batch = tickers[i:i+5]
                
                try:
                    # ì§€ë‚œ 7ì¼ê°„ íŠ¸ë Œë“œ
                    pytrends.build_payload(batch, timeframe='now 7-d')
                    interest = pytrends.interest_over_time()
                    
                    if not interest.empty:
                        # ê° í‹°ì»¤ì˜ í‰ê·  ê´€ì‹¬ë„ (0-100)
                        for ticker in batch:
                            if ticker in interest.columns:
                                avg_interest = int(interest[ticker].mean())
                                trends_data[ticker] = avg_interest
                    
                    # Rate limit ë°©ì§€
                    import time
                    time.sleep(2)
                    
                except Exception as e:
                    print(f"âš ï¸ {batch} íŠ¸ë Œë“œ ì¡°íšŒ ì‹¤íŒ¨: {e}")
                    continue
            
            print(f"âœ… Google Trends ë¶„ì„ ì™„ë£Œ: {len(trends_data)}ê°œ í‹°ì»¤")
            return trends_data
            
        except Exception as e:
            print(f"âš ï¸ Google Trends ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {}
    
    def analyze_weekly_hot_news(self) -> List[Dict]:
        """ì£¼ê°„ í•« ë‰´ìŠ¤ TOP 10 ë¶„ì„"""
        print(f"\n{'='*60}")
        print(f"ğŸ”¥ ì£¼ê°„ í•« ë‰´ìŠ¤ TOP 10 ë¶„ì„ ì‹œì‘")
        print(f"{'='*60}\n")
        
        # 1. ì§€ë‚œ 7ì¼ ë‰´ìŠ¤ ê¸°ë¡ ë¡œë“œ
        weekly_news = self._load_weekly_news_history()
        
        if not weekly_news:
            print("âŒ ë¶„ì„í•  ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return []
        
        # 2. Reddit WSB ë¶„ì„
        reddit_data = self.get_reddit_wsb_hot_tickers(limit=100)
        wsb_tickers = reddit_data.get('tickers', {}) if reddit_data else {}
        wsb_contexts = reddit_data.get('contexts', {}) if reddit_data else {}
        
        # 3. Google Trends ë¶„ì„ (ìƒìœ„ 20ê°œ í‹°ì»¤ë§Œ)
        top_tickers = list(wsb_tickers.keys())[:20] if wsb_tickers else []
        trends_data = self.get_google_trends_data(top_tickers)
        
        # 4. GPTì—ê²Œ ì¢…í•© ë¶„ì„ ìš”ì²­
        print(f"\nğŸ¤– GPT-4o-minië¡œ ì¢…í•© ë¶„ì„ ì¤‘...\n")
        
        # ë‰´ìŠ¤ ë°ì´í„° ì¤€ë¹„
        news_summary = "\n\n".join([
            f"[{idx+1}] {news.get('title', '')}\nìš”ì•½: {news.get('summary', '')[:200]}\në°œì†¡: {news.get('sent_at', '')[:10]}"
            for idx, news in enumerate(weekly_news[:100])  # ìµœëŒ€ 100ê°œ
        ])
        
        # Reddit ë°ì´í„° ì¤€ë¹„
        reddit_summary = ""
        if wsb_tickers:
            reddit_summary = "Reddit r/wallstreetbets í•« í‹°ì»¤:\n"
            for ticker, count in list(wsb_tickers.items())[:15]:
                context = wsb_contexts.get(ticker, {})
                trend = trends_data.get(ticker, 0)
                reddit_summary += f"- {ticker}: {count}íšŒ ì–¸ê¸‰"
                if trend > 0:
                    reddit_summary += f" | Google ê²€ìƒ‰: {trend}/100"
                if context:
                    reddit_summary += f"\n  ëŒ€í‘œê¸€: {context.get('title', '')[:80]}"
                reddit_summary += "\n"
        
        # GPT í”„ë¡¬í”„íŠ¸
        prompt = f"""ë‹¹ì‹ ì€ ê¸ˆìœµ ë‰´ìŠ¤ ì „ë¬¸ ì• ë„ë¦¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤.

ì§€ë‚œ 7ì¼ê°„ì˜ ë¯¸êµ­ ì£¼ì‹ ë‰´ìŠ¤ì™€ ì†Œì…œ ë°ì´í„°ë¥¼ ì¢…í•© ë¶„ì„í•˜ì—¬ **ì£¼ê°„ í•« ì´ìŠˆ TOP 10**ì„ ì„ ì •í•´ì£¼ì„¸ìš”.

**ì§€ë‚œ 7ì¼ê°„ ì „ì†¡ëœ ë‰´ìŠ¤** ({len(weekly_news)}ê°œ):
{news_summary}

**ì†Œì…œ ë¯¸ë””ì–´ ë¶„ì„**:
{reddit_summary if reddit_summary else "ì†Œì…œ ë°ì´í„° ì—†ìŒ"}

---

**ì„ ì • ê¸°ì¤€** (ìš°ì„ ìˆœìœ„):
1. **ë°˜ë³µ ë“±ì¥ ì£¼ì œ**: ì—¬ëŸ¬ ë‚ ì— ê±¸ì³ ë°˜ë³µëœ ì´ìŠˆ (ì˜ˆ: ì—”ë¹„ë””ì•„ê°€ ì›”/ìˆ˜/ê¸ˆ ë“±ì¥)
2. **Reddit í™”ì œì„±**: WSBì—ì„œ ë§ì´ ì–¸ê¸‰ëœ ì¢…ëª©/ì´ìŠˆ
3. **Google ê²€ìƒ‰ íŠ¸ë Œë“œ**: ê²€ìƒ‰ëŸ‰ì´ ë†’ì€ ì¢…ëª©
4. **ì‹œì¥ ì˜í–¥ë„**: ì§€ìˆ˜, ì„¹í„°, ê±°ì‹œê²½ì œì— í° ì˜í–¥
5. **íˆ¬ìì ê´€ì‹¬ë„**: ì‹¤ì , M&A, ê·œì œ ë“± ì¤‘ìš” ì´ë²¤íŠ¸

**ì œì™¸ ê¸°ì¤€**:
- ì¼íšŒì„± ì†Œê·œëª¨ ë‰´ìŠ¤
- ë°˜ë³µ ì—†ëŠ” ë‹¨ë°œì„± ì´ìŠˆ
- Reddit ë°ˆ/ë†ë‹´ ì„±ê²©

**ì‘ë‹µ í˜•ì‹** (JSONë§Œ):
{{
  "weekly_hot_topics": [
    {{
      "rank": 1,
      "title": "ì£¼ì œ/ì¢…ëª©ëª… (í•œêµ­ì–´)",
      "summary": "ì´ë²ˆ ì£¼ ë¬´ìŠ¨ ì¼ì´ ìˆì—ˆëŠ”ì§€ 3-4ë¬¸ì¥ ì¢…í•© ìš”ì•½ (í•œêµ­ì–´)",
      "frequency": "3ì¼ ë“±ì¥" ë˜ëŠ” "Reddit 234íšŒ" ë“±,
      "heat_score": 95,
      "related_tickers": ["NVDA", "AMD"]
    }}
  ]
}}

**ì¤‘ìš”**: 
- ì œëª©ê³¼ ìš”ì•½ì€ ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ì‘ì„±
- TOP 10ê°œë§Œ ì„ ì •
- heat_scoreëŠ” ì¢…í•© ì ìˆ˜ (1-100)
- ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬

JSONë§Œ ì¶œë ¥í•˜ì„¸ìš”."""

        try:
            response = requests.post(
                'https://api.openai.com/v1/chat/completions',
                headers={
                    'Authorization': f'Bearer {self.openai_api_key}',
                    'Content-Type': 'application/json'
                },
                json={
                    'model': 'gpt-4o',  # GPT-4o ì‚¬ìš© (ì£¼ê°„ ë¶„ì„)
                    'messages': [
                        {'role': 'system', 'content': 'ë‹¹ì‹ ì€ ê¸ˆìœµ ë‰´ìŠ¤ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”.'},
                        {'role': 'user', 'content': prompt}
                    ],
                    'temperature': 0.3,
                    'max_tokens': 3000
                },
                timeout=120  # GPT-4o íƒ€ì„ì•„ì›ƒ
            )
            
            if response.status_code != 200:
                print(f"âŒ GPT ë¶„ì„ ì‹¤íŒ¨: {response.status_code}")
                return []
            
            result = response.json()
            response_text = result['choices'][0]['message']['content']
            
            # JSON ì¶”ì¶œ
            if "```json" in response_text:
                json_start = response_text.find("```json") + 7
                json_end = response_text.find("```", json_start)
                response_text = response_text[json_start:json_end].strip()
            elif "```" in response_text:
                json_start = response_text.find("```") + 3
                json_end = response_text.find("```", json_start)
                response_text = response_text[json_start:json_end].strip()
            
            analysis = json.loads(response_text)
            hot_topics = analysis.get('weekly_hot_topics', [])
            
            print(f"âœ… ì£¼ê°„ í•« ë‰´ìŠ¤ TOP {len(hot_topics)}ê°œ ì„ ì • ì™„ë£Œ\n")
            
            # ê²°ê³¼ ì¶œë ¥
            for topic in hot_topics[:5]:
                print(f"   {topic['rank']}. {topic['title']} (ì ìˆ˜: {topic.get('heat_score', 0)})")
            
            return hot_topics
            
        except Exception as e:
            print(f"âŒ GPT ë¶„ì„ ì˜¤ë¥˜: {e}")
            return []


def main():
    """í…ŒìŠ¤íŠ¸ìš© ë©”ì¸ í•¨ìˆ˜"""
    openai_api_key = os.getenv('OPENAI_API_KEY')
    sent_news_file = '/data/sent_news_history.json' if os.path.exists('/data') else 'sent_news_history.json'
    
    if not openai_api_key:
        print("âŒ OPENAI_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        return
    
    analyzer = WeeklyHotNewsAnalyzer(openai_api_key, sent_news_file)
    hot_topics = analyzer.analyze_weekly_hot_news()
    
    if hot_topics:
        print("\n" + "="*60)
        print("ğŸ”¥ ì£¼ê°„ í•« ë‰´ìŠ¤ TOP 10")
        print("="*60 + "\n")
        
        for topic in hot_topics:
            print(f"{topic['rank']}. {topic['title']}")
            print(f"   {topic['summary']}")
            print(f"   ë¹ˆë„: {topic.get('frequency', 'N/A')} | ì ìˆ˜: {topic.get('heat_score', 0)}")
            if topic.get('related_tickers'):
                print(f"   ê´€ë ¨: {', '.join(topic['related_tickers'])}")
            print()


if __name__ == "__main__":
    main()
